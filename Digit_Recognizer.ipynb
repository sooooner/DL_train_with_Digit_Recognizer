{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DIGIT Recognizer](https://www.kaggle.com/c/digit-recognizer)\n",
    "<img src=\"images/front_page.png\" style=\"width:350px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load & Pretreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "41995       0    ...            0         0         0         0         0   \n",
       "41996       0    ...            0         0         0         0         0   \n",
       "41997       0    ...            0         0         0         0         0   \n",
       "41998       0    ...            0         0         0         0         0   \n",
       "41999       0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "27995       0       0       0       0       0       0       0       0       0   \n",
       "27996       0       0       0       0       0       0       0       0       0   \n",
       "27997       0       0       0       0       0       0       0       0       0   \n",
       "27998       0       0       0       0       0       0       0       0       0   \n",
       "27999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "27995       0    ...            0         0         0         0         0   \n",
       "27996       0    ...            0         0         0         0         0   \n",
       "27997       0    ...            0         0         0         0         0   \n",
       "27998       0    ...            0         0         0         0         0   \n",
       "27999       0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "27995         0         0         0         0         0  \n",
       "27996         0         0         0         0         0  \n",
       "27997         0         0         0         0         0  \n",
       "27998         0         0         0         0         0  \n",
       "27999         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfY = df_train[\"label\"]\n",
    "dfY = pd.get_dummies(dfY)\n",
    "dfY = dfY.as_matrix().reshape(len(dfY), -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = df_train.drop(\"label\", axis = 1)\n",
    "dfX /= np.max(np.max(dfX))\n",
    "dfX = dfX.as_matrix().reshape(len(dfX), -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test /= np.max(np.max(df_test))\n",
    "df_test = df_test.as_matrix().reshape(len(df_test), -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = (dfX - np.mean(dfX)) / np.std(dfX)\n",
    "df_test = (df_test - np.mean(df_test)) / np.std(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 42000), (10, 42000), (784, 28000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dfX), np.shape(dfY), np.shape(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAHiCAYAAADxt5d3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b94adce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "for i in range(12):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.imshow(dfX[:, i].reshape(28,28))\n",
    "    plt.axis('off')\n",
    "    plt.title(np.where(dfY[:, i] == 1)[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAHVCAYAAADoyKBiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XdgVMX68PGz2RSSUBNqaKGF3kRQlC5goQgCcsUC2BVE\nUbFf9V69NlRAih28XhQRUFRUECyodFGK1NCltwChpO3u+8d7fzPn2ZsNyya72dl8P389w8yePbDJ\nw+TJnBmHx+OxAADmiSruGwAABIYEDgCGIoEDgKFI4ABgKBI4ABiKBA4AhiKBA4ChSOAAYCgSOAAY\nKjqUb9YjahCPfRaThe5ZjmBdm8+1+PC5RiZ/P1dm4ABgKBI4ABiKBA4AhiKBA4ChSOAAYCgSOAAY\nigQOAIYigQOAoUjgAGAoEjgAGIoEDgCGIoEDgKFCuplVuHCWLatiR0J8QNc4fE1dFSfftMfnOMdo\n/V7utZsCei8AyA8zcAAwFAkcAAxVIksom8Y2VPHW3m8F9b2uKX+7ivnfEkBRIqcAgKFI4ABgqBJT\nQsnq3U7Fb18xLWTv23niMhUfzC4n+raMbqziqF/XhOyeSgJnw/qifahzpZC9d9wpfRJZmU+Wh+x9\nS7qoUqVUvPe+i0TfncO+VvG95XeKvsOusyquFl1axfW+Hy7GNRqzX8V5Bw8V7maLCDNwADAUCRwA\nDEUCBwBDlZga+MCXF6i4a3xWyN730eQNPvu+nKqfzJxyzyDRF/396qDdUyT568nLVJxd0a3ipAbH\nxbjFrcb5db0Yh1PFuR5XQPf0e46uxQ5vd7foq/6zvsf4uSsDuj40Z/06uvFutg5rTxTjbn/7PhV/\n/UOm6Is6dU7Fe/pVVnG5y4+JcYmz81R8skNg91vUmIEDgKFI4ABgqBJTQpn59FUqbjn2bRW3j/Pv\nx+SWb94n2rUWZPoYaVk7++qlSN8PHaviKk65cVbfxAwVj7lOfhRpi3Xbk5dnlWRH72yv4qi+8sfa\nT5q9ruKGMYUvfxSFS+JyVbz2+vGib3KP5iqen9VFxbHzVwX9viJB9tVtRXv8lEkqfmJXfxX/o/9N\nYlz1tUtV7LEk+1dK9Ze3qdhZoYIY12LxARUvTU6R1zgmS3ahwgwcAAxFAgcAQ5HAAcBQJaYGnjhn\nhYqfct2p4sMXOfMb/j9Svz4p2p4/fC8PTLWVM9/te4l+34rrfL5mS78pon3ts7pm7zpyxK97jFQn\nGumq5dpWHxbjnRTeiArrVTyrRncVJxfHzRjC2UAfnvLExA9E3wv7rlGxq+t+W89+q7D2DWss2i3i\ndQ5ZkhO6rRkKwgwcAAxFAgcAQ5WYEoqd/Qm42nP9e4330iN/LX5MPyn41Hu+SyglXVRioop3PtJS\n9G0cPMHW8l3yynTnqHjayRYB3cf8g01VHN3d91mnBfG01/f/1ez3AroGtEOv6zQV65DLQ88MiinS\n9zp546UqnvvAK6Kv99uPqLhG5lIrHDADBwBDkcABwFAlsoQSSnEZ2ecfBMvVQh/A8Nttr4u+XD/r\nV/ayyaJmZQK6j2grsLKJuMax0yoeubeL6BtX/XsVH7tYlwMqz5XrUFxH5ROnJc2x2/XTtwtavari\nAfeMFuNKHSj8hmBnBuqVYq89p1eD9ZnyiBhX4+XwKJvYMQMHAEORwAHAUCRwADAUNfAgO3hp6fMP\nQkRxbd2u4t/fby87n9Y18PW931DxgKl3yHElvAZ+vJU++OLDk3oHx1LzAqt5O6vogxq2PFpX9I3r\no5/uHbl+iIprvP6bGBfoUuJgYgYOAIYigQOAoSihBFm/4YuL+xYiSvN5o0Q7+Tf9ZGZspv4ht4y1\nPGT3VJAqPx0W7c59blHx4tZmb8wVKuWcZ22tsqLPWUlvKnW2baqKd/eT12jfVB/U0DBHLhW1P92Z\n8oA+HzMvN8cKd8zAAcBQJHAAMBQlFC9Zfdqp+HhD/c8T5XXEYtVxvp/K8lzeSsWtE2b79b4j93WQ\nf5Bdsp7gvGna1z77mn83UsWNn9ou+sL9iUX7ihTLsqwT6XqzJKu1Dq//90Ix7tPGVYN5W2Gv6q8O\nFfe5Vv8bVko/JcZVj9arUlrE6nJa//TeYty+lxqo+LkJ74i+0S/do+KKO5cFeMfFgxk4ABiKBA4A\nhiKBA4ChjK6BO8uXE21HUgUV7xqcIvrij+glZmnDN/u85rAq01TcNT5LxbkeWQS/feCVPq/RM/kb\nFfdKOOlz3PiMNBX/dWM10ec6tcPn6yLRjWX0crvNXtsPJqTHqjjca97e7MvcLMuyPBX10rQYh67Z\n3lT2LzHuU6tk18DLfKKXgV4b9ZCKD16RJ8bFHNYHOqT8ovvivpVPUe7/qIqKfz+XKvoqvlf4HQ2L\nCzNwADAUCRwADGVGCeVSvVH/rt767MRKFx8Sw35sPitot2D/cdeyLOvfqYsKfc2aMcdVvH1oFdFX\n94WDKnafPWtFujxLl6huXjdM9NV4Mfw20i/I0Tv1BlbHL5Y/8q/vNlHF9krRgC0Dva6yNxi3ZqSy\nHy+3xf695sg9chOxzZ0nq7jT6HtFX2l3eDy1Gwhm4ABgKBI4ABiKBA4AhjKiBr6zr657bxg6KaBr\nHHXpXcZmZjZTcUpMhhjXP/G4FSoDSh/V8XD592rVWO9aV/tuvcTOdeRI8G+smA1MXSPa8/t1VnH8\n3PBY8mWvc59oJJc9bhysD2rwXn7q0+MVvP6AGviFcrTVBz98+fhY0dd0iX5cvvasFSG7p2BjBg4A\nhiKBA4ChjCihbBqqlwC5CxhnN3RXd9Fe/3ljFae8qpelOZteIsatnr5Fxc9XXu3Xe+3MyxLtXp88\nnO+4SzpuEu1ptb/Pd5xlWdaaS/Vm/1dM10vM4q+M/BLKqKS1ou18Tn/q87O6iL7Y+asK9V5b320r\n2lWq65Kay+17fvNE2kcqvjLhsFev0/KH/XCKxju8dln06wqIKlVKxVd+8IuKPz3VQoyre5cuSbk8\n4Xi6ZWCYgQOAoUjgAGAoI0ooTof+f8bt52/1n6z+jWhvuPN33bjT3rNTjGsTt8/Wivd5/SVZehOd\nJ54YIfrqzsx/U/iMqvJpyx4fDlDx3+t9Jfo6ldKbHn3fTB8K0dtq4/OeItWICutVXP8N+fTtjhy9\nWVSM7WzDXI9/ZYz3y44T7UrOONs1ir6Q4etwCtM26QoX++++SMXdEl9X8UM33S3GRWXIlU2Rghk4\nABiKBA4AhiKBA4ChjKiBN15ys4rXXfaBX69Ji4n1avv7hKWuez9/VC9Fmj2zsxiVtFnXR8t85t9u\nZnkHZf02rqeO/3HtbaLv44m6ntd9ue0pMmu9FYn6DLxdxV/Nfs/nuN6JXrViWzvatnwvz++FeHGy\n5YjxMc636adqivbH+/TB2NHd94i+NEsfNMBSwQvnad9StL968BUV91imdxlM/TUya97emIEDgKFI\n4ABgKIcnhE8l9YgaFNCb2Z+2ctTQZ0e63s4t9D05R3otFTx6QsfZ2fq9Tp0q9HtdCGfFZBV7Tp9R\nsTsrK7/h57XQPctR6JvyIdDP1c6ZVk/FB1+VlT375lbeT2na2Q/dCHQJ4O48/VfxPljCl6oPya9D\nV3rozjMN98+1KNjPvr16yS6f476+SJ8j6rF97wbKnnf+/43ory/3mTNWMPn7uTIDBwBDkcABwFAk\ncAAwlBHLCEXdd5vt0fcrCn/tcF3KVdIerXZt1Y+VV+or++wHOkzt0NXnNdwVdS16XffJPsd1XD1M\nxZnp5UVf3DE9p/H3MOVw/RqKFHs/SFFx/zJfi747+tyhYk+23O2zsDZPaC7ad7T/WcWffCCTT41p\n+r1dGfKQmGBiBg4AhiKBA4ChjCihoGSzn4NZb67vcfall136jfY5rtpi/URs5XT/nqJFaGX11k+z\nLm4zXsXdXnpEjKu81r8yVyAaP71btN99VpfyUnoeFH1HuuldMZN6U0IBAJwHCRwADEUJBRHDvnIn\n+b38D9WwLFaNhCNHnNxUrNHTetO2Nj/oQzAaTA5eycSb65A86zTtHu+zT4sfM3AAMBQJHAAMRQIH\nAENRAwdQ7KJqpoj2iMozVLxrZP1Q344xmIEDgKFI4ABgKEooAIqdy75JnWVZY1IvtbUi8xzYosAM\nHAAMRQIHAEORwAHAUCRwADAUCRwADEUCBwBDOTweT3HfAwAgAMzAAcBQJHAAMBQJHAAMRQIHAEOR\nwAHAUCRwADAUCRwADEUCBwBDkcABwFAkcAAwFAkcAAxFAgcAQ5HAAcBQJHAAMBQJHAAMRQIHAEOR\nwAHAUCRwADBUdCjfrEfUIM5vKyYL3bMcwbo2n2vx4XONTP5+rszAAcBQJHAAMBQJHAAMRQIHAEOR\nwAHAUCRwADAUCRwADEUCBwBDkcABwFAhfRITCBVncpJoH+nbUMWJNxxQ8bSG08W4D05couIv3u8s\n+lI+3KBi14mTRXKfQGEwAwcAQ5HAAcBQJHAAMBQ1cEQMZ9myKs6YXkH0LW8xWcVuy77JXrwY91TF\ndSp++tH1ou+tu2ur+OsBl6rYtSk9oPsFCosZOAAYigQOAIaihIKIsfWZJireZCuZWJZlnfbkqLj1\nFw/4vMaoLgtUfF/5HaLvznK7VPztm81U7OpyoXcKFA1m4ABgKBI4ABiKEgoihivB7bOvy4sPqbjB\n5KU+xy0oVVXFb7x6pejb0n+Kij+q/5mKh1TrL8blHTh4/psFigAzcAAwFAkcAAxFAgcAQ1ED9+Jo\n21zFnlW2J/GinGKcs5x+6i+vSW3Rt7dbQr7XTp19RLR5gq9opd2zUsX9xg4QfZV3+K5727mzslTc\n+KW9ou+j7tVUfHMZXefe9KT8/BuMpAaen1NDLhXtR/+hd4Lsk3DK5+ua/DrMZ1/C4tIqLrc7V//5\nCrkE1HX0mL+3aRRm4ABgKBI4ABiqxJdQtv2ntWgPb7lMxYse66jiA5fJf6oJf5uq4h7x34s+uVmS\n9vr1jUT7h+aJF3az8Fvejl2FvkZOncqiXSla/5hv/4y7XrxBjJOFF/yfY80cot0rQR+Kcc6TK/oO\nufJUvLHDByr+n++tDvm/15dn5GZmR/LKqHjqzstEX9lXdZ9z8Vrd4Xblf/EwwgwcAAxFAgcAQ5HA\nAcBQJbIGfuAhXQN7s/27ou+K+GwV//5YTRW7z5UW4x4ff5uKH5OlPSvbdp7uw3/Tj1w/nLRFjJs6\nc5iKUwevs3DhxOHFbl0fdWVkFP7aKzeK9tqzerlgz3jdt+zrFmJcTcu/JYslTV71bJ99Q7b3k2Pv\n1nXpo+0qqvhkffm6nBp6l8maKcd9Xv/+Ovr3VEtafSI7bedaX7u1j4r/+jZVDEt5Jfw+V2bgAGAo\nEjgAGKpEllBqztyt4rsaDRN9TV44rGL3Uf0jWXymfIoy3trp13vN/E7vaHfL7Kmi788O01Tc22rj\n1/UguY7pzyi6eoqKnVUrynEBPPXqrCSvMSZZ/wi9LkcvMaux6MwFX7ukcHfUy3RndXrLq1c/3bxp\nX1XRU3fTGhVXsH12cnGg/95r2UvFL7cuL/ouHvGHir9I+0p3pMlrNG6ly6YNbt0s+uxP8IYSM3AA\nMBQJHAAMVWwllKhm8qlE95+bfYwsGtGptVS88fEqKk7YESPG5e3cbRUldynn+QehSOTt268b+2Sf\ns2Kyijc/q5cyxFY+K8a5durVRu8OfFv0RVl6udHNq29Vcc1lay3kb89IXWpqEev7eyFqZ3xQ78O9\ndpOKK3h9XLsX6PJNp64jVPzic++IcVs66xJo0ydGiL7aTy+zigMzcAAwFAkcAAxFAgcAQxVbDTzY\nNW9vrqm6FreiwXgVD29/vRiXZxWt46N9LzH79HRln30oWulj9JqwLf0n+R54uQ7tNW/LsqwbdvZQ\nce3h+nclvo9Shp33v+eaHP3dVm/8NtEXyn0A7YdQV5infyey4vF6YlyXeL2csVyYnMXCDBwADEUC\nBwBDReyTmPZlY5ZlWZv/0ksHr333IRWX3bu8yN/bfq7m3FZv2nrkUqnrS+unPj+0aloInjpfnlPx\nP65speJnKq3Jb3i+MrL1WadRmZF5xmJRc67TyzJXtZWHMdw0534V1ztSPMvwvO2aqpcbf5H0o+hr\nv1aXW5Pm/ilfmGD72kjSz4u6jhwVwzzZvjf0CgQzcAAwFAkcAAwVsSWU9IflTjSru72m4oEf3lek\n72XfsMeyLGvA29+puLpT/2j1/qkaYtzcPpfYWv5tjoXAOJboUsnqjnozo1Yj5dfC4nvHqrhClCx5\nfd5wlorbPfWgims+H377RIeLmv/S/zbP/Etu2FbPuvCyiaN1U9He07ucip226kRy9/1inMOhyzce\nj9cG/jYP1Vzks+/XFvrzv+HbHqIvJV6f7zm26pcq7nHH3WJc3DerfF4/EMzAAcBQJHAAMBQJHAAM\n5fB4POcfVUR6RA0K2Ztdv+mgaOd69E5onzepVOjr25cKdp0qlyI+mKSfMj3q0svXbr5llBjn/On3\nQt+Hvxa6Z/ku/BVSKD/XYDs15FIVL3tVHkDg8uhnLv+TqXew+7RDcznuaOiWGJa0z7XjOnlwwqPJ\nG/x63c9ZsSou5cgVfe3i8v9rPnawrWivfkrX8BPX7fMerpxpUV2/1w/yrFt/lxH6+7kyAwcAQ5HA\nAcBQEbWM0F7W6JH4q+gb8qB++jLRWuHX9exnLG76VzXR91O3CSq2LxW0LMv6+mwZFU8aqjf+dy4N\nXckEgSn7sS6H1elxm+jb2lNv8H9jmQMqfvnegWJcrX+yrDBY3l/WUbR/b6yfYP5jc6qKa8+Vr0vc\neEjF6S/KkzU3dNIHNQzefpWKz42Spda4NXoJYEGb3sXZDhYJdg2KGTgAGIoEDgCGIoEDgKEiqgZe\nkP2ddNxgto6dFWQ9bNsUvRvZuLYzVdwzXh7MkOHWq3wa/iRrpXUn6eVmDg68LRb2319YlmVtv6u2\nih2NT6u4zp17xThXRoaKm/xdLkW1eub/Xo6wW2wXudLuko+i278r06wjPl93ppdeEvhF+wmi760T\njVV84p/6+z9mzeoA7zJ0mIEDgKFI4ABgqIgqoXhWrVfxwjP1Rd/q/uNUfGNzvezrgZoLxbiu8fJJ\nr//zzslU0Z7+XC8V15tR9IdCwD/O5CQVp49pqOKPBr8hxlWK0k/AjegxVMX2kkmgon0fe4piEpWY\nKNpvTtZlk8QoeYrpvNs6qzhmefiXTeyYgQOAoUjgAGCoiCqh2E1O7yzaw9rop6O+SpunYqdD/h/m\nsm323mm9LrWUv11uQhOMszRxfo42ckP/6lN2qfirGpNVfMotNywadLPeSMy5VT8RG121ihh3omOq\nins+9Yvoi7L018aSbP11U/2HE2Kc/AEdoWIvp0V/Fiv60mJKqbj+F/KQhbTlK4N7Y0HEDBwADEUC\nBwBDkcABwFARWwOvfN120b74zpEqzk6ydXhtm15nut6ovexR/WRXXmZmkd4f/Gc/yLamreZtWZY1\npcbPKrbXnnfkyS/ttLEbVezy6MOK/151jhhXxan7ory+ODLc+nCOUePH6NesYffBcOCuqw9SmFP/\nA9E354z+pm/y3B7RV9DOguGOGTgAGIoEDgCGitgSiidP/mBUeYp/P+aa/ONUpDresqyKv64hl/b5\nWrLXItYp2hNT9OfvFtvsx1u+vHisiWjPe6WLiqtMp2wSbraNdvrse+WVISpOPrAsFLcTEszAAcBQ\nJHAAMBQJHAAMFbE1cESO5E/+UHFay3tF34ge36n4vgrpPq/x1VldRx+/s7uKD54oI8bFLtft6lPk\nIdTlstg+IdxkDGuv4i2d9VYKS7JlPTz5vcipe9sxAwcAQ5HAAcBQlFAQ9txZ+pCN+qNlGWOBVdYW\nt/HrenHWLhXX9j2MXQXDUZQsjbgGHFOxfXnoXf+WpbZaVmQu+2QGDgCGIoEDgKEooQAwxpG72on2\niosmqXhnni611f5Gbj7nsSITM3AAMBQJHAAMRQIHAENRAwdgjNM1ffctOK13j/SsWh+Cuyl+zMAB\nwFAkcAAwFCUUAMZIXi8XBA7efpWK0+c1UHFKhD556Y0ZOAAYigQOAIYigQOAoaiBAzBG2RlyN8oz\nM3ScYh0J8d0UP2bgAGAoEjgAGMrh8UTqPl0AENmYgQOAoUjgAGAoEjgAGIoEDgCGIoEDgKFI4ABg\nKBI4ABiKBA4AhiKBA4ChSOAAYCgSOAAYigQOAIYigQOAoUjgAGAoEjgAGIoEDgCGIoEDgKFI4ABg\nqJCeSt8jahDntxWThe5ZjmBdm8+1+PC5RiZ/P1dm4ABgKBI4ABiKBA4AhiKBA4ChSOAAYCgSOAAY\nigQOAIYigQOAoUjgAGAoEjgAGIoEDgCGIoEDgKFCupmVCaKrp6h47+RyKv6t7XQxLsbhVHGuxyX6\nmv57pIpTluSpOH7fGTHOvWZj4W4WgiMuTsXbn7tI9LkS3Cq+5pI1Kp6Qsszn9ZZky/nNPe/eq+Ka\nY1eq2JOXZwHFgRk4ABiKBA4AhqKE4uVI99oqXnbxBBXnFrAzsncJZc0t+nXWLTp84tBlYtzGUa1U\n7FiyxsKFiypTRsXHPqmq4o2tJvl+jaW3WnZbvj/Y9nFen+vIiSruvHOEist8sty/m0VQZfVup+IT\nDWRqizmtP+cKW7NE3+GL4vO9Xvl0WRorNW9lvuOKEzNwADAUCRwADFXiSyjOismifel9vwXtvV6o\nslS2J2er+PeB9UWfa9vOoN1HRKldXYU3p/peUWK3Jkf/aLw6K1X0jfvzChXPa/em6KsVrX/Ufuy5\nD1X81tpeYpxrU7pf94ELd3C0LEM2H6RXct1SeZqKu8bLMskh1zkV/3qupujrm3go3/damV1KtB+7\n9zoVJw0/Jfpchw4XdNtBwwwcAAxFAgcAQ5HAAcBQDo+ngPVxRaxH1KDQvZmf7E/vWZZl7XxaP8H3\nx9AJ3sOVo+4cFX+W2Uz0XZmo63K1o/37NcOnp2uI9oxGKT5GBmahe5bj/KMCEy6fq7NpQxVXe3+f\n6Ptxs+5Lm6w/O8+q9T6vd/heWW9d+eTEfMddM+R20Y5a/Mf5b7aIlITP1V73bjpwk+h7r/aCfF9j\nf1Lasv53qa8/CrrGrbuvEn2bZjVScdXx8nddgfD3c2UGDgCGIoEDgKFK/DLCA/e0Ee0/ho7363Xd\nPx6j4jqPyeVrc78bouJvmnzq1/XqxnovQyraEkpJ4NqwRcV7L5V9DazVKva3LhBzJiwqCCXS9rHt\nVfzT4FdUnBQVK8ZtytXx2P26rPGf1IV+v5eva1xefrsYd1s5vTx0au35ou/4g1+quN9pnRuS3/Nv\naWugmIEDgKFI4ABgKBI4ABiqRNbADzyolyVNH/W6V69//6d5173t8t6pouK9r+kCWw1njM/XpEaf\nFu2/ntL3WPP5wi9LQgCuP1rcd1Bi7B8jl2xuGqKXbOZ6dN27x59/E+M80yqruMxM266QchWp4O81\nvml1uRg3+bprVPz7rXKJsb02n5sYtJWd/4MZOAAYigQOAIYqMSWUqMRE3eiUocL60b7/D9vr0uWP\nG//xsOhLsnyXUBJnr1DxcMeDKl44Lv8n+SzLsip6LY+q2XWPiqMm6EML3JmZPq+BC2c/EMKyLGvP\nB7VUPKPp+16jdQnsx3N6p7qYo2fFqAt/5q9kcHduLdrbr9df81v6ye8N+1OQe/L0ToK5n1QR4yrM\nzP/7sHf1Nvn+uWVZVmlrh9efeLf/e79eZ9bWtp250rzMKNG3+frJuhG6CgozcAAwFQkcAAwVsSUU\nZ3KSaG96tY6KN1z8pvdw5cessip+9l/DVZw0LbAnqsrsPBPQ6z5v+JmKWz9yv4pT/x7cJ7si1aFR\nepXDg/fqp2NjHbLgMaD0T7aW71VDjWJ1Ge5gJ/m1VmlDYPcYiRwxukyy7Wa5OdSGq+wrTeTr7GWT\nAa89ouIqH4TJiiyv+7VvdDX8jm9U/O348kG9DWbgAGAoEjgAGIoEDgCGitga+KaxdUV7Q48pfr3u\nxW36aatA6952zn36ab6Of9wk+n5pPb3Q1y/pHK2bqnh3n3IqfnyI3AVycBn95Fy0pWuxbr/3JpSq\nOfUBx85ex2Sn71+xlDhRabbfPV3l3/egZVnWDc/oHf3Cpu7tp1l/6UNh/nfJYtFiBg4AhiKBA4Ch\nIraEMqPL28V9C5ZlWVbegYMqzv1JlnWs1hYuUFTLxqJ9+8yvVNw3McN7uP2VQbojy1rU6t+i/bcW\nevmpe93moL1vuIquU1vF8W8dK2CkdtH7D4h2bcPKJnanv62qYkooAIB8kcABwFARVULJWah/dGsT\nt9qrV688mHaqporn7L9IjEq8Kng/8ni8Nrmxb9hToBBujhPuyk05JNr9Ek/YWv79Qzkdet7Sad11\noi/hef0kbuw+WZLZ8Yru+/MyXTYp7YgT4/ZeqZ/MTFnn1y1FlL39qqt4WR3fZ8y+daKRiut8Jv+t\n3UV/W4UWXUP/vW664pdivBONGTgAGIoEDgCGIoEDgKGMroHnXHmxaF9T7UcV23cH8zb5nX4qrjou\ndMuVHAXsYOZt2ql6Kq6yiiMC/s+pOyqK9vMzmqm4fWK6z9c9vfVaFWf+os9ArPGi78/fFSdr2y1T\n9GEaBT3BGZMZ2NOdkaig3/Msul5//7o3bgrF7VyQqFZNRPuaj3Xd+85yu7xG2/6eHOgAADgfEjgA\nGMroEsr+TnLD/RHlw+/HMGeaLoX0vdn30qNteXLh1JxRPVUcv2hl0d+YoVwbt4r20pb6wIClVlPv\n4Uo5a1u+cUHy2svr/Sf13XzH7czLEu2qP+unD0ti8evOO/TTsfYyYeslt4txdXdvD9k9BWLHgHKi\nPbysvl/vAyheOKrP4Kwxd6+K84JzawozcAAwFAkcAAxFAgcAQxlXA7cvHfzmprFevbGWL21X6B3i\nar29RsXBeGTXXvceNm+RinsnHPH5mhPuUqIds8h7KwCE2plqvr+e7G5cP1y0k7zq9CXNjD1tVTy8\nma4bu/YmiHHuM4Ed+B0qf946SbS96952n83pqOKau0K3NJkZOAAYigQOAIYyroTiidaPOVVz+vcj\nrmVZVk7UstokAAAPq0lEQVS2XnLoPnu20PcRnVpLxX8NqCH67MsFCyqb2N2x6hbRTrVK4DZ2YcDR\nRi8drDNySzHeibkyF+gDDSz9oKw1+7oJYtzIZaNUnDh7RbBvy6ecq3TJ564Js/16TdNvRoh2w5d+\nU3Eon8NlBg4AhiKBA4ChjCuhhNLRO9urOLOb/I15nUrHVby8oe9N6wvS6pc7VFx/1H7RVxKf4AsV\nR4wuvR0Z3kb0vfaoPkv18lK5Pq8xev9lKo5/r3wR3l3kSouRuzw9/dJUFb9wbpjoi/t6VZG+t/0w\nhh3jkkRfnYr7VNw30X5giO+NuJyZss+Tm1O4GwwQM3AAMBQJHAAMRQIHAEOVmBr4rPa6trlkY32/\nXtM2/g0VN4uRi4PsG9UX9IRWtkfvR3bp0rtFn73u7Tri33JDeGnXXMcr16vQ0VruJHikrT6Q+GRn\nvXvgpi7yabuCrMzWNdxt96apOH4Vu0XalU/XX/Pd1g1R8S8tZ4pxXeP159D1nbdE38h9HVS8d7A+\ngCOnlqxfb79e/z5jS78pKvY+SCLX4++Tzfp1MzKriJ5nftIHYKc9tNzP6wUXM3AAMBQJHAAMZVwJ\nJe6I/rFrYob8Mfm+Cht8vs6+hCktJrgbyT98oJOK52/Q99hgmPwxjqWCWsZQvWTz3sfn+P26JnF6\nudnGbL1UrFGs/LduYzveMsp2aGFBm5lNzGgg2ov6t1KxJ32993D8V6l5uqSUuFVv7LZhgTzewHtZ\nod2k6r+q+Pm5LVTcuJRcbmtf9ldQKbOg82ftfsvRJRl7ycSyLCvt7vArlTEDBwBDkcABwFAkcAAw\nlHE1cPtSsQUPdRZdUzt2V/E3t8jDHi5k58ILNWzX1aKdOUIve2qwloMZ/HGq12kV31jmwAW8Us9B\nWsdeyOvyd+nvN6i46t2nRZ9r345CX7+kcW3Vv28aNfo+0XfoBv37rD8uf8/nNR5N/qNI7+nW3VeJ\n9m/L9ZLQOnOzVZz2S/jVvL0xAwcAQ5HAAcBQ5pVQbGK++020U7/T8eDdY0Tfz8/KzeQv1CVvPCDa\nyev1TnUJu0+KPvfGTYV6r5Ko3mOZKn7rq7qi7+7yF166eOxgW9H+fKluO3L18rWG/5SfVaVzu1Wc\nl51toejEz5UliboLE1U8sPZNoq/8u/rJZLfle7mhL6tWpYl2w3f07qHWkQzRV+9IeDxVGQhm4ABg\nKBI4ABjK4fGE7gS3HlGDQnlcHGwWumdd+M+hfuJzLT58rpHJ38+VGTgAGIoEDgCGIoEDgKFI4ABg\nKBI4ABiKBA4AhiKBA4ChSOAAYCgSOAAYigQOAIYigQOAoUjgAGAoEjgAGCqkuxECAIoOM3AAMBQJ\nHAAMRQIHAEORwAHAUCRwADAUCRwADEUCBwBDkcABwFAkcAAwFAkcAAxFAgcAQ5HAAcBQJHAAMBQJ\nHAAMRQIHAEORwAHAUCRwADAUCRwADBUdyjfrETWI89uKyUL3LEewrs3nWnz4XCOTv58rM3AAMBQJ\nHAAMRQIHAEORwAHAUCRwADAUCRwADEUCBwBDkcABwFAkcAAwFAkcAAxFAgcAQ5HAAcBQJHAAMFRI\ndyMsTqcHXaLirCT9/1bu1SfEuFebzVZxz4RcFbs8bjGu+8b+Kt5/vJzoS/oiQcXxh/U1YhatvtDb\nBgCfmIEDgKFI4ABgqIgtoZz6tp5o/9RikooXnNUlj0Unm4px8060UvE3J/V+9m6P3F/944Yfqbii\nM16+eQcdnvXkqLj1Zw+IYQ0f/1Nf/8yZ//k7IPSiU2up+GyjKqJvl66aWS920aW2v5XJEOPmnC6r\n4vcvaSP6XBlyLFAYzMABwFAkcAAwVMSWUBqUPyLarafcr+I6U3eoOO/AwYCuf1uTW1XsiZX/jKca\nlFHxof7ZKl533QQxrm21O1Rca9D6gO4DWvbVbVVc+tG9oq9j8ja/rtE6/lsVd43P8us1uV4nR16Z\ncFjFUxO8ymuUUArFERMr2rv+rktU7XvqkuS0Wr+Icbkel4qb/Tpc9Ll3Jaq4ztyz+r2WrS3czYYA\nM3AAMBQJHAAMRQIHAENFbA38UPtTol3TWqrivCK4vmvjVp99pdfouNzGhio+3kG+8+hm36t4jlW5\nCO6qZNs/VP++YUODeUF9r/EZaSqesqSb6Ks1Ty85LbVvZVDvI1I5K1VSsaNUnIrPNKsmxq297Y18\nX5/rkXPTobu6q/j2pktE36gOm3XjJh22mDZKjEt9alnBN10MmIEDgKFI4ABgqIgtoRSnqGaNVHz8\nFV02qeb1xOarX1yr4jpW+P14ZgLPZS1V/G37ybYe+W+9N++cig+5vJb22bx9uIuKF//WRPTVnaM3\nJotds1PFaRmr/L1d2ESVKqXiXY9cJPqy6+slnBfV2aPibYtlymr6yX0qLr9Rl66qfPeXGOc6rJcV\n/xhfQ/TN6dVDxQteHqfiqUMmi3H/mnGDvt6GLVY4YAYOAIYigQOAoUjgAGAoauABcpbXOxpue1TW\nSjfdomtnTof+P7LDukFiXJ3HqHsXVtYzerlorWhd2/7xXCkx7rUhw3RjZUHbFmSqqIG1wucol88e\n+Ov01fr3F2vukttMHHLpJaG33ajr3Km/+Pc9U9BSYVd2tmgnrdRbH6zI0jtJem+lcLqB/p6P3+DX\nbQQdM3AAMBQJHAAMRQmlAFGtdGlkd6/you+lYR+o+OqEH0TfWydTVTzt9d4qrjRjnRgnT9lEUXpy\nc3/RTiqwbILiUOZH/QRk3wG3ib6oc3rJZtTaP0J2T6ZhBg4AhiKBA4ChSnwJ5fSgS0Q7Y7A+m/Kr\ntm+pOEEeiWldt2Goih9eKzeiqv/SRhUnn9C/NadkUnj2TY4sy7JebDAn33HZP1T0+hPfm4/Znb7+\nUhWX2yAPXwiXp+8ihevESd1YXnzlxcxm+mvKvvJkSVaMGFd6w1EVh8sqJGbgAGAoEjgAGIoEDgCG\nitgauH2nM8uyrG3/bK3iXlfo3ePuq/iaGGd/mu/yNfrw0/L/ShDjyi7VB56WtbaLvnCpj0Ui++b+\nlmVZ7eI8+Y7z2s/fSp+sf9fxWLevVNwxXh52XDtaH8Bw0p0j+rq9/4iK60zW9XDX0WPnuWuEsxP1\nnCp226rvt359hxjXIN33k7nFhRk4ABiKBA4AhorYEsrm8S1Ee2ufST5G+t7cf1mrmSoe/HJP0bd6\nfTsV150jiybR36/28y4RLL+PnujnyDifPRW9DuBYd6e+ZqsWt6i4xgBKKCaJrlFdtIcPm6/iI7ZN\ntBo+uVGMC8dlwMzAAcBQJHAAMFTEllCSfneK9rUN+1zwNaIceoXD8JQlom9m3e9sF5evm3U6WcVj\nX/ubiiu+w/7fhZW3d59o99x4nYq/a/KZX9dYl6NLXs/tkV8X6fPrqTiuvSyNzG89VcX1Kuo+ubs0\nwl2dz4+K9ogKekVRp8fHqLh8Zvh/vzIDBwBDkcABwFAkcAAwlMPjyf9JtmDoETUodG9WxJxly4r2\n6W6NVLy3vzyBb27nKSquHa3/ym1+vkeMqzdkTVHeYoEWumc5zj8qMMX5uZ7tr5+w3NdfHwIQs0cu\nD6y5SFeqYw+fVrFro3+7FFqWZaVP1O81v+/rKn5k13Vi3LnOh/y+ZmFF6uda1HY9317FK4e9Lvqe\nOdRRxVs6xqrYffZs8G/MB38/V2bgAGAoEjgAGIoSShAcHnmZimePeUXF3odC9HvStmTpP8FdssSP\n2oVn3yCt2VJdkrm/4i9i3PWPPKziMp8sD+o98bnmL6tPO9Fe9NabKv7yTAXR996gXip2r90U3Bvz\nEyUUAIhwJHAAMBQJHAAMFbGP0henypOWqvjKVF0P3XTDZDEuZ6Dt0Nz/BP22UEjuLH3g7ec/6MOP\nX7jhNzEuathh3fgk6LeF/4pq1UTFj4/7t+izH9Tw/Bs3ib7Ka5dapmIGDgCGIoEDgKEooQRZxSZH\nffbVLq9LKOdCcTMoMpVX2Ro3FNttoF1zFd4+/QsVXxEvn6JsMn2UiutOCb+zLQPFDBwADEUCBwBD\nUUIpAvYn9CzLsvY8eJGKN7TSG1ttyMkR43JuSwjujSFoMgaeUXGUJR+ay8zSG2klhuyOSghbycSy\nLGvER3NUfGXCSRWPOdhejEub9JeK89zyDFuTMQMHAEORwAHAUCRwADAUNXA/RdeoLtr7rqut4roD\n00XfmnoTVezy6Ppon59GinENtq0uylsskaLr6M/h2OQYFZeLyxLjjn9UU8VJUwPb+dHTvqWKJ100\nXcXrc3LFuBr36wMj5FEfCIT9CUv7UkHLknXv1441U3F674piXN6BvUG6u+LFDBwADEUCBwBDlcgS\nSnT1FBW7KsvN3ff2LKfiRr31eYkjU74V41rG6h+TS0fJ8xe/P6eXB947f5i+3nM7xbjIWcxUfA6+\nof/tl7ec4XNc81Rdvkry89rRqbVE++QzmSruVEovCf3xXGkxLm/3XxYKZ+/j+lCU1257X8XeT1h2\nWT9YxaWv2mHrORi0ewsnzMABwFAkcAAwFAkcAAxlXA3cEa1v2VmrhujbeaOubZ+rJhdwXXvJ7yru\nUe4HFfeMP2P5Yn9E2m3J812zPfr/vqG7uou+E9fp5WwNDumdz6h5F73E2NzzD7Is69o+eungd4d1\nfdXrY7VONtSf0tRe74q+y0vl/14zj17i9Sen8x0H33Y9Lx99/3O4XoprP4yh+ZJbxbh6D+kdPUvi\nkk1m4ABgKBI4ABjKuBLK4c/qqXhFm499jhu5r4NoP1V5sYqP65/IrNXZcifB8Qd6qHjjkSoqzv1D\nLjestEb/wBb/xcrz3DWC5dhP1VR8tqle2pfgiBXjnq+sn3p9/vHCPwF7//7LVbzj2caiL9Za5T0c\n+Tg9SJeeVg573atXf35PHWqn4nqjj4lRefv2B+XeTMEMHAAMRQIHAEMZV0Kp1HeLintbbQoYKU+Z\nvNG63Mc4b8dVVM0WIzzVeHGpiju4HlJxY9vXiWVZ1kd1viv0ezWeMULFDV/bpeLYA5RMApE7TH9/\nlXLIVDTmoC6vpPerquK8fZG5KVWgmIEDgKFI4ABgKBI4ABjKuBo44EvKK7oefvIV2Vfw70v8U89a\nruKS+NRfUTu+OVnFr9VsJvrsBzJE6mEMRYEZOAAYigQOAIaihAKgWNR7SJekFlvxXr0l40CGwmIG\nDgCGIoEDgKFI4ABgKBI4ABiKBA4AhiKBA4ChHB6P5/yjAABhhxk4ABiKBA4AhiKBA4ChSOAAYCgS\nOAAYigQOAIYigQOAoUjgAGAoEjgAGIoEDgCGIoEDgKFI4ABgKBI4ABiKBA4AhiKBA4ChSOAAYCgS\nOAAYigQOAIYigQOAoUjgAGAoEjgAGIoEDgCGIoEDgKH+HxQ+QBQOqGAHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b94adcef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "for i in range(12):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.imshow(df_test[:, i].reshape(28,28))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4132 4684 4177 4351 4072 3795 4137 4401 4063 4188]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b94aafc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(list(range(10)), np.sum(dfY, axis=1), palette='Greens_d')\n",
    "print(np.sum(dfY, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check None Value\n",
    "# assert(dfX.isnull().any().count() == np.shape(dfX)[1])\n",
    "# assert(dfY.isnull().any().count() == np.shape(dfY)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(dfX, dfY):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dfX -- input dataset of shape (input size, number of examples)\n",
    "    dfY -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_hi -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "\n",
    "    n_x = dfX.shape[0]      # size of input layer\n",
    "    n_h1 = 128\n",
    "    n_h2 = 64\n",
    "    n_h3 = 64\n",
    "    n_y = dfY.shape[0]      # size of output layer\n",
    "    layer_dims = (n_x, n_h1, n_h2, n_h3, n_y)\n",
    "    \n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 128, 64, 64, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dims = layer_sizes(dfX, dfY)\n",
    "layer_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layer_dims):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    layer_dims -- output of layer_sizes()\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h1, n_x)\n",
    "                    b1 -- bias vector of shape (n_h1, 1)\n",
    "                    W2 -- weight matrix of shape (n_h2, n_h1)\n",
    "                    b2 -- bias vector of shape (n_h2, 1)\n",
    "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
    "                    b3 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)   \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he(layer_dims)\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape W1 : (128, 784)\n",
      "shape b1 : (128, 1)\n",
      "shape W2 : (64, 128)\n",
      "shape b2 : (64, 1)\n",
      "shape W3 : (64, 64)\n",
      "shape b3 : (64, 1)\n",
      "shape W4 : (10, 64)\n",
      "shape b4 : (10, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in parameters.keys():\n",
    "    print(\"shape {} :\".format(i), np.shape(parameters[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 42000)\n",
      "Z = [[-0.10228664 -5.9642208  -0.43389473 ..., -2.60185159 -2.41329243\n",
      "  -1.52039894]\n",
      " [ 2.82584809  0.81807472 -1.14001134 ..., -1.04033409  0.45633203\n",
      "   0.0748405 ]\n",
      " [-0.65174132  1.07281173 -0.99581919 ..., -1.08544987 -0.56923968\n",
      "  -1.60624157]\n",
      " ..., \n",
      " [-1.43458059 -0.33789312 -0.36951042 ..., -0.23611781 -1.11874615\n",
      "  -0.9343949 ]\n",
      " [-1.1928547   3.41546009 -0.86262532 ..., -0.5409382   0.31365376\n",
      "  -0.47623487]\n",
      " [ 0.4701796  -3.09390347 -0.41718899 ..., -1.71826461 -2.38829194\n",
      "  -1.85956903]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = dfX, parameters['W1'], parameters['b1']\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(Z.shape)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47445061,  0.00256246,  0.39319669, ...,  0.06901935,\n",
       "         0.08216468,  0.17940278],\n",
       "       [ 0.94405673,  0.6938275 ,  0.24231828, ...,  0.26108554,\n",
       "         0.61214367,  0.5187014 ],\n",
       "       [ 0.34259724,  0.74513126,  0.26976421, ...,  0.25247607,\n",
       "         0.36141228,  0.16711108],\n",
       "       ..., \n",
       "       [ 0.19238598,  0.41632135,  0.40865933, ...,  0.44124328,\n",
       "         0.24624393,  0.28203394],\n",
       "       [ 0.23274876,  0.96818422,  0.29679113, ...,  0.36796936,\n",
       "         0.57777685,  0.3831416 ],\n",
       "       [ 0.61542627,  0.04335943,  0.3971896 , ...,  0.15209483,\n",
       "         0.08406986,  0.13475329]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(Z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU activation in numpy.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.82584809,  0.81807472,  0.        , ...,  0.        ,\n",
       "         0.45633203,  0.0748405 ],\n",
       "       [ 0.        ,  1.07281173,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  3.41546009,  0.        , ...,  0.        ,\n",
       "         0.31365376,  0.        ],\n",
       "       [ 0.4701796 ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(Z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implement the softmax activation in numpy.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.38415497e-03,   4.63491941e-06,   2.48005737e-03, ...,\n",
       "          1.45534628e-04,   2.73615890e-04,   7.17976964e-04],\n",
       "       [  6.32590986e-02,   4.08842191e-03,   1.22405006e-03, ...,\n",
       "          6.93625055e-04,   4.82395651e-03,   3.53927409e-03],\n",
       "       [  1.95355260e-03,   5.27456424e-03,   1.41390761e-03, ...,\n",
       "          6.63027037e-04,   1.72982929e-03,   6.58915201e-04],\n",
       "       ..., \n",
       "       [  8.92981234e-04,   1.28684209e-03,   2.64498661e-03, ...,\n",
       "          1.55021332e-03,   9.98517367e-04,   1.29005679e-03],\n",
       "       [  1.13716256e-03,   5.49017065e-02,   1.61534911e-03, ...,\n",
       "          1.14290373e-03,   4.18252936e-03,   2.03978880e-03],\n",
       "       [  5.99886619e-03,   8.17721027e-05,   2.52183657e-03, ...,\n",
       "          3.52130212e-04,   2.80542647e-04,   5.11458984e-04]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(Z)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear activation forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\" or \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)  \n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)   \n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = np.tanh(Z), Z\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)  \n",
    "\n",
    "    return A, cache     # g(Z), ((A, W, b), g(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = dfX, parameters['W1'], parameters['b1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1019314 , -0.9999868 , -0.40857104, ..., -0.98906774,\n",
       "        -0.98409974, -0.90876716],\n",
       "       [ 0.99300146,  0.67402062, -0.81441791, ..., -0.77801996,\n",
       "         0.42709001,  0.07470109],\n",
       "       [-0.57284104,  0.79051819, -0.75983273, ..., -0.79521138,\n",
       "        -0.51480068, -0.92260269],\n",
       "       ..., \n",
       "       [-0.89260148, -0.32559516, -0.35356341, ..., -0.23182552,\n",
       "        -0.80713234, -0.73263637],\n",
       "       [-0.83146212,  0.9978426 , -0.69760783, ..., -0.49369782,\n",
       "         0.3037574 , -0.44322318],\n",
       "       [ 0.43834442, -0.99589977, -0.39455967, ..., -0.93765373,\n",
       "        -0.98329131, -0.952639  ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation_forward(A_prev, W, b, activation='tanh')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the LINEAR -> RELU -> LINEAR  TANH -> LINEAR -> SIGMOID -> LINEAR -> SOFTMAX computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_he()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "#     for l, act in zip(range(1, L), ['relu', 'tanh', 'sigmoid']):\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "#         A, cache = linear_activation_forward(A_prev, parameters['W{}'.format(l)], parameters['b{}'.format(l)], activation = act)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W{}'.format(l)], parameters['b{}'.format(l)], activation = 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W{}'.format(L)], parameters['b{}'.format(L)], activation = \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return AL, caches    # A[4], ((A[0], W1, b1, (Z1)), (A[1], W2, b2, (Z2)), (A[2], W3, b3, (Z3)), (A[3], W4, b4, (Z4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 42000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0769263 ,  0.07520451,  0.14874251, ...,  0.04118916,\n",
       "         0.06077849,  0.11304059],\n",
       "       [ 0.13301316,  0.04549142,  0.02048419, ...,  0.00475803,\n",
       "         0.0734537 ,  0.11928567],\n",
       "       [ 0.07089929,  0.00875754,  0.0365227 , ...,  0.01041019,\n",
       "         0.09523808,  0.06560213],\n",
       "       ..., \n",
       "       [ 0.04113581,  0.03967157,  0.02875263, ...,  0.05381069,\n",
       "         0.06353326,  0.04422012],\n",
       "       [ 0.17019676,  0.30524569,  0.14447443, ...,  0.11734197,\n",
       "         0.24674606,  0.21256956],\n",
       "       [ 0.06959346,  0.05049661,  0.04252543, ...,  0.0180457 ,\n",
       "         0.07324914,  0.02411527]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A4, caches = L_model_forward(dfX, parameters)\n",
    "print(A4.shape)\n",
    "A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\hat{y},y) = -\\frac{1}{m}\\Sigma_jY_jlog\\hat{y}_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{m}\\Sigma_{i=1}^mL(\\hat{y^{(i)}},y^{(i)}) + \\frac{\\lambda}{2m}\\Sigma_{l=1}^L\\|W^{[l]}\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, caches, Lambd=1):\n",
    "    \"\"\"\n",
    "    Implement the cost function.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability matrix, output of the forward propagation (L_model_forward())\n",
    "    Y -- ture\"label\" numpy ndarray\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    Lambd -- regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    L2_regularization_cost = (Lambd * np.sum([np.sum(caches[i][0][1]**2) for i in range(len(caches))])) / (2*m)\n",
    "    cross_entropy_cost = -1/m * np.sum(np.sum(Y * np.log(AL), axis = 1, keepdims=True)) \n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8349683871792428"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = compute_cost_with_regularization(A4, dfY, caches, Lambd=1)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backword propagation \n",
    "+ input : $da^{[l]}$ \n",
    "+ output : $da^{[l-1]}, dW^{[l]}, db^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dZ^{[l]} = da^{[l]} * g^{[l]'}(Z^{[l]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[l]} = dZ^{[l]} * a^{[l-1]} + \\frac{\\lambda}{m}W^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ db^{[l]} = dZ^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[l-1]} = W^{[l]T} \\bullet dz^{[l]}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dZ^{[l]} = W^{[l+1]T} \\bullet dZ^{[l+1]} * g^{[l]'}(Z^{[l]}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, Lambd=1):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    Lambd -- regularization parameter, integer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1 / m * np.dot(dZ,A_prev.T) + Lambd * W /m\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-layer (softmax)\n",
    "+ differential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a^{[4]}_i = P_i = \\frac{e^{Z^{[4]}_i}}{\\Sigma_i e^{Z^{[4]}_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g^{[4]'}_i(Z^{[4]}_i) = \\frac{\\partial g(Z^{[4]}_i)}{\\partial Z^{[4]}_i} = P_i(1 - P_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g^{[4]'}_j(Z^{[4]}_i) = \\frac{\\partial g(Z^{[4]}_i)}{\\partial Z^{[4]}_j} = - P_i * P_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ input\n",
    "$$ da^{[4]} = \\frac{\\partial L}{\\partial a^{[4]}_i} = P_i - y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dZ^{[4]}_i = \\frac{\\partial L}{\\partial a^{[4]}_i}\\frac{\\partial a^{[4]}_i}{\\partial Z^{[4]}_i} = (P_i - y_i){P_i(1 - P_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[3]} = W^{[4]T} \\bullet dZ^{[4]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[4]} = dZ^{[4]}_i * a^{[3]} + \\frac{\\lambda}{m}W^{[4]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ db^{[4]} = dZ^{[4]}_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single Softmax unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    p, _ = softmax(Z)\n",
    "    dZ = dA * p * (1 - p)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-layer (sigmoid)\n",
    "+ differential\n",
    "$$ a^{[3]}_i = \\frac{1}{1 + e^{Z^{[3]}_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g^{[3]'}(Z^{[3]}_i) = \\frac{\\partial g(Z^{[3]}_i)}{\\partial Z^{[3]}_i} = \\frac{1}{1 + e^{Z^{[3]}_i}}{(1 -  \\frac{1}{1 + e^{Z^{[3]}_i}})} = a^{[3]}_i(1 - a^{[3]}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ input\n",
    "$$ da^{[3]} = \\frac{\\partial L}{\\partial a^{[3]}_i} = W^{[4]T} \\bullet dZ^{[4]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dZ^{[3]}_i = \\frac{\\partial L}{\\partial a^{[3]}_i}\\frac{\\partial a^{[3]}_i}{\\partial Z^{[3]}_i} = W^{[4]T} \\bullet dZ^{[4]}_i * a^{[3]}_i(1 - a^{[3]}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[2]} = W^{[3]T} \\bullet dZ^{[3]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[3]} = dZ^{[3]}_i * a^{[2]} + \\frac{\\lambda}{m}W^{[3]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ db^{[3]} = dZ^{[3]}_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s, _ = sigmoid(Z)\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-layer (tanh)\n",
    "+ differential\n",
    "$$ a^{[2]}_i = tanh(Z^{[2]}_i) = \\frac{1 - e^{Z^{[2]}_i}}{1 + e^{Z^{[2]}_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g^{[2]'}(Z^{[2]}_i) = \\frac{\\partial a^{[2]}_i}{\\partial Z^{[2]}_i} = (1 - \\frac{1 - e^{Z^{[2]}_i}}{1 + e^{Z^{[2]}_i}})(1 + \\frac{1 - e^{Z^{[2]}_i}}{1 + e^{Z^{[2]}_i}}) = (1 - a^{[2]}_i)(1 + a^{[2]}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ input\n",
    "$$ da^{[2]} = \\frac{\\partial L}{\\partial a^{[2]}_i} = W^{[3]T} \\bullet dZ^{[3]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ output\n",
    "$$ dZ^{[2]}_i = \\frac{\\partial L}{\\partial a^{[2]}_i}\\frac{\\partial a^{[2]}_i}{\\partial Z^{[2]}_i} = W^{[3]T} \\bullet dZ^{[3]}_i * (1 - a^{[2]}_i)(1 + a^{[2]}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[1]} = W^{[2]T} \\bullet dZ^{[2]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[2]} = dZ^{[2]}_i * a^{[1]} + \\frac{\\lambda}{m}W^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ db^{[2]} = dZ^{[2]}_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single Tanh unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    t = np.tanh(Z)\n",
    "    dZ = dA * (1 - t) * (1 + t)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-layer (relu)\n",
    "+ differential\n",
    "\n",
    "$$ a^{[1]}_i = max({0, Z^{[1]}_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g^{[1]'}(Z^{[1]}_i) = \\frac{\\partial a^{[1]}_i}{\\partial Z^{[1]}_i} =\\begin{cases}\n",
    "0, & \\mbox{if }Z^{[1]}_i \\le 0 \\\\\n",
    "1, & \\mbox{if }Z^{[1]}_i > 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ input\n",
    "$$ da^{[1]} = W^{[2]T} \\bullet dZ^{[2]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ output\n",
    "$$ dZ^{[1]}_i = \\frac{\\partial L}{\\partial a^{[1]}_i}\\frac{\\partial a^{[1]}_i}{\\partial Z^{[1]}_i} = \\begin{cases}\n",
    "0, & \\mbox{if }Z^{[1]}_i \\le 0 \\\\\n",
    " W^{[2]T} \\bullet dZ^{[2]}_i, & \\mbox{if }Z^{[1]}_i > 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ da^{[0]} = W^{[1]T} \\bullet dZ^{[1]}_i  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[1]} = dZ^{[1]}_i * a^{[0]} + \\frac{\\lambda}{m}W^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ db^{[1]} = dZ^{[1]}_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation, Lambd=1):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"tanh\" or \"softmax\"\n",
    "    Lambd -- regularization parameter, integer\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, Lambd)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, Lambd)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, Lambd)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache, Lambd)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA_prev, dW, db = linear_activation_backward(dA=A4 - dfY, cache=caches[3], activation='softmax')\n",
    "# dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, dfY, caches, Lambd=1):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR -> RELU -> LINEAR  TANH -> LINEAR -> SIGMOID -> LINEAR -> SOFTMAX group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- ture\"label\" numpy ndarray\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"softmax\" (it's caches[L-1])\n",
    "    Lambd -- regularization parameter, integer\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)         \n",
    "    m = AL.shape[1]\n",
    "    dfY = dfY.reshape(AL.shape) \n",
    "    \n",
    "    dAL = AL - dfY\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\", Lambd=Lambd)\n",
    "    \n",
    "#     for l, act_func in zip(reversed(range(L-1)), ['sigmoid', 'tanh', 'relu']):\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "#         dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l + 1)], current_cache, activation = act_func)\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l + 1)], current_cache, activation = 'relu', Lambd=Lambd)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = L_model_backward(A4, dfY, caches)\n",
    "# grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, s = initialize_adam(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(Y, parameters, grads, learning_rate, Lambd=1):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = (1 - ((learning_rate * Lambd) / m)) * parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = update_parameters(dfY, parameters, grads, learning_rate=0.02)\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update parameters with adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V_{dW^{[l]}} = \\beta_1 V_{dW^{[l]}} + (1 - \\beta_1) dW^{[l]} , \\beta_1 : momentom $$\n",
    "$$ V_{dW^{[l]}}^{corrected} = \\frac{V_{dW^{[l]}}}{1-(\\beta_1)^t} $$\n",
    "\n",
    "$$ S_{dW^{[l]}} = \\beta_2 S_{dW^{[l]}} + (1 - \\beta_2) (dW^{[l]})^2 , \\beta_2 : RMSprop $$\n",
    "$$ S_{dW^{[l]}}^{corrected} = \\frac{S_{dW^{[l]}}}{1-(\\beta_2)^t} $$\n",
    "\n",
    "$$ W^{[l]} := W^{[l]} - \\alpha \\frac{V_{dW^{[l]}}^{corrected}}{\\sqrt{S_{dW^{[l]}}^{corrected} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V_{db^{[l]}} = \\beta_1 V_{db^{[l]}} + (1 - \\beta_1) db^{[l]} , \\beta_1 : momentom $$\n",
    "$$ V_{db^{[l]}}^{corrected} = \\frac{V_{db^{[l]}}}{1-(\\beta_1)^t} $$\n",
    "\n",
    "$$ S_{db^{[l]}} = \\beta_2 S_{db^{[l]}} + (1 - \\beta_2) (db^{[l]})^2 , \\beta_2 : RMSprop $$\n",
    "$$ S_{db^{[l]}}^{corrected} = \\frac{S_{db^{[l]}}}{1-(\\beta_2)^t} $$\n",
    "\n",
    "$$ b^{[l]} := b^{[l]} - \\alpha \\frac{V_{db^{[l]}}^{corrected}}{\\sqrt{S_{db^{[l]}}^{corrected} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(Y, parameters, grads, v, s, t = 2, learning_rate = 0.01, Lambd=1, \\\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2   \n",
    "    m = Y.shape[1]\n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                         \n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - (beta1 ** t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - (beta1 ** t))\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - (beta2 ** t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - (beta2 ** t))\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = (1 - ((learning_rate * Lambd) / m)) * parameters[\"W\" + str(l+1)] - \\\n",
    "                                     learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "                                     learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_adam, v, s = update_parameters_with_adam(dfY, parameters, grads, v, s, t=1, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8)\n",
    "# parameters_adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(dfX, dfY, mini_batch_size = 128, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    dfX -- input dataset of shape (input size, number of examples)\n",
    "    dfY -- labels of shape (output size, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)           \n",
    "    m = dfX.shape[1]                  \n",
    "    mini_batches = []\n",
    "        \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = dfX[:, permutation]\n",
    "    shuffled_Y = dfY[:, permutation]\n",
    "\n",
    "    num_complete_minibatches = int(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size ]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, mini_batch_size * num_complete_minibatches : m]\n",
    "        mini_batch_Y = shuffled_Y[:, mini_batch_size * num_complete_minibatches : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches # [(minibatch_X, minibatch_Y), x num_complete_minibatches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(dfX, dfY, mini_batch_size = 128, seed = 0)\n",
    "len(mini_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(dfX, dfY, learning_rate = 0.0075, mini_batch_size = 64, num_iterations = 3000, print_cost=False, Lambd=1):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR -> RELU] x 3 -> LINEAR -> SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    dfX -- data, numpy ndarray of shape (num_px * num_px, number of examples)\n",
    "    dfY -- labels of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- bool, if True, it prints the cost every 100 steps\n",
    "    Lambd -- regularization parameter, integer\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(0)\n",
    "    costs = []\n",
    "    \n",
    "    layers_dims = layer_sizes(dfX, dfY)\n",
    "    \n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "    \n",
    "    mini_batches = random_mini_batches(dfX, dfY, mini_batch_size, seed=0)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        for mini_batch in mini_batches:    \n",
    "            \n",
    "            (minibatch_X, minibatch_Y) = mini_batch\n",
    "            \n",
    "            AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "            cost = compute_cost_with_regularization(AL, minibatch_Y, caches, Lambd=Lambd)\n",
    "\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches, Lambd=Lambd)\n",
    "            \n",
    "            parameters = update_parameters(dfY, parameters, grads, learning_rate, Lambd)\n",
    "            \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i : %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model_adam(dfX, dfY, learning_rate = 0.0075, mini_batch_size = 64, num_iterations = 3000, print_cost=False, \\\n",
    "                       adam=False, Lambd=1, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, seed=0):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR -> RELU] x 3 -> LINEAR -> SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    dfX -- data, numpy ndarray of shape (num_px * num_px, number of examples)\n",
    "    dfY -- labels of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- bool, if True, it prints the cost every 100 steps\n",
    "    Lambd -- regularization parameter, integer\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    costs = []\n",
    "    \n",
    "    layers_dims = layer_sizes(dfX, dfY)\n",
    "    \n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "    \n",
    "    if adam:\n",
    "        t = 0\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    mini_batches = random_mini_batches(dfX, dfY, mini_batch_size, seed=seed)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        for mini_batch in mini_batches:    \n",
    "            \n",
    "            (minibatch_X, minibatch_Y) = mini_batch\n",
    "            \n",
    "            AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "            cost = compute_cost_with_regularization(AL, minibatch_Y, caches, Lambd=Lambd)\n",
    "\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches, Lambd=Lambd)\n",
    "            \n",
    "            if adam:\n",
    "                t = t + 1\n",
    "                parameters, v, s = update_parameters_with_adam(dfY, parameters, grads, v, s, t, learning_rate, Lambd,\\\n",
    "                                                               beta1, beta2,  epsilon)\n",
    "            else:\n",
    "                parameters = update_parameters(dfY, parameters, grads, learning_rate, Lambd)\n",
    "            \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration %i : %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 : 16.964849\n",
      "Cost after iteration 10 : 10.873290\n",
      "Cost after iteration 20 : 7.065330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-8cf20676e817>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(dfX, dfY, learning_rate, mini_batch_size, num_iterations, print_cost, Lambd)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost_with_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-c7b7dced6357>\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         A, cache = linear_activation_forward(A_prev, parameters['W{}'.format(l)], parameters['b{}'.format(l)], activation = act)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-04a9a3943a64>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-c380909fee89>\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(A, W, b)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parameters = L_layer_model(dfX, dfY, learning_rate = 0.01, mini_batch_size = 128, num_iterations = 300, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in maximum\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less_equal\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 : nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-2ef83ef638e6>\u001b[0m in \u001b[0;36mL_layer_model_adam\u001b[1;34m(dfX, dfY, learning_rate, mini_batch_size, num_iterations, print_cost, Lambd, beta1, beta2, epsilon, seed)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost_with_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-24791cf3c053>\u001b[0m in \u001b[0;36mL_model_backward\u001b[1;34m(AL, dfY, caches, Lambd)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#         dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l + 1)], current_cache, activation = act_func)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mdA_prev_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA_prev_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-de473b0caf4c>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, cache, activation, Lambd)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-4f63b698dac8>\u001b[0m in \u001b[0;36mlinear_backward\u001b[1;34m(dZ, cache, Lambd)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLambd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parameters = L_layer_model_adam(dfX, dfY, learning_rate = 0.01, mini_batch_size = 128, num_iterations = 100, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, _ = L_model_forward(df_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(pd.DataFrame(test).idxmax(), columns=['Label'])\n",
    "test.index.name='ImageId'\n",
    "test.index += 1 \n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê°œì„ ì‚¬í•­ \n",
    "+ ~~L2 Regularization~~\n",
    "+ ~~Normalizing inputs~~\n",
    "+ ~~He initialization~~\n",
    "+ ~~Mini-batch~~\n",
    "+ ~~Adam optimization~~\n",
    "+ Dropout vs Batch Normalizing\n",
    "+ Hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
